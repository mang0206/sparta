###
 curl -X POST http://host.docker.internal:8080/v1/chat/completions
  -H "Content-Type: application/json"
  -d '{"model": "ollama", "messages": [{"role": "user", "content": "도커 연결 테스트"}], "stream": false}'
###
POST http://host.docker.internal:8080/v1/chat/completions
Content-Type: application/json

{
  "model": "ollama",
  "messages": [
    {
      "role": "user",
      "content": "도커 연결 테스트"
    }
  ],
  "stream": false
}

###


POST http://host.docker.internal:8080/v1/chat/completions
Content-Type: application/json

{
  "model": "ollama",
  "messages": [
    {
      "role": "user",
      "content": "도커 연결 테스트"
    }
  ],
  "stream": false
}

###



###
POST http://localhost:8080/v1/chat/completions
Content-Type: application/json

{
  "model": "ollama",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "안녕하세요!"}
  ],
  "temperature": 0.7,
  "max_tokens": 500,
  "stream": false
}
###
POST http://localhost:8080/v1/chat/completions
Content-Type: text/event-stream

{
  "model": "ollama",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "!"}
  ],
  "temperature": 0.7,
  "max_tokens": 500,
  "stream": false
}

###
GET http://localhost:8080/v1/chat/models
Content-Type: text/event-stream

{
  "model": "anthropic",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "!"}
  ],
  "temperature": 0.7,
  "max_tokens": 500,
  "stream": true
}
